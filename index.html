<!DOCTYPE html>
<!--
GROQ Prompt:

write a short JS tool, where user post a question to ai-models team. first will give concise, complete reply to the answer and save in a conclusion, then second reads that and correct or accept it, then third do the same, and it keeps running unless all accept the answer OR everyone gets 3 turns. Show final result as conclusion.

keep ui modern and slick with team chat mode style - properly format the output of each reply.

All will use one groq key '[your-groq-api-key]' for all, just change models. 
Also test all model are getting correct output.

models:
openai/gpt-oss-120b
meta-llama/llama-4-scout-17b-16e-instruct
qwen/qwen3-32b
groq/compound

make one file solution.
-->

<html lang="en">
<head>
<meta charset="UTF-8">
<title>AI Models Team Chat</title>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
<style>
  :root {
    --bg:#f7f9fc;
    --card:#fff;
    --primary:#4f46e5;
    --secondary:#64748b;
    --radius:12px;
    --shadow:0 4px 12px rgba(0,0,0,.08);
  }
  body{margin:0;background:var(--bg);font-family:'Inter',sans-serif;display:flex;flex-direction:column;height:100vh;}
  .chat{flex:1;overflow-y:auto;padding:1rem;display:flex;flex-direction:column;gap:0.75rem;}
  .msg{max-width:70%;padding:0.75rem 1rem;border-radius:var(--radius);box-shadow:var(--shadow);line-height:1.4;}
  .msg.user{align-self:flex-end;background:var(--primary);color:#fff;}
  .msg.bot{align-self:flex-start;background:var(--card);color:#111;}
  .msg .model{font-size:0.75rem;font-weight:600;margin-bottom:0.25rem;color:var(--secondary);}
  .input-bar{display:flex;padding:1rem;background:#fff;box-shadow:var(--shadow);}
  .input-bar input{flex:1;padding:0.75rem 1rem;border:1px solid #e2e8f0;border-radius:var(--radius);font-size:1rem;}
  .input-bar button{margin-left:0.5rem;padding:0 1.5rem;background:var(--primary);color:#fff;border:none;border-radius:var(--radius);cursor:pointer;}
  .final{background:var(--secondary);color:#fff;padding:1rem;margin:1rem;border-radius:var(--radius);}
  .msg table,.final table{width:100%;border-collapse:collapse;margin:0.5rem 0;}
  .msg th,.msg td,.final th,.final td{border:1px solid #e2e8f0;padding:0.4rem 0.6rem;text-align:left;}
  .msg th,.final th{font-weight:600;}
</style>
</head>
<body>

<div class="chat" id="chat"></div>

<div class="input-bar">
  <input type="text" id="questionInput" placeholder="Ask the team a questionâ€¦" />
  <button id="sendBtn">Send</button>
</div>

<script>
/* ==================== CONFIG ==================== */

// replace this with your groq key: https://console.groq.com
const GROQ_KEY = '[your-api-groq-key]';
const MODELS = [
  { name: 'meta-llama/llama-4-scout-17b-16e-instruct', turn:0, accepted:false },
  { name: 'openai/gpt-oss-120b',               turn:0, accepted:false },
  { name: 'groq/compound',                    turn:0, accepted:false },
  { name: 'qwen/qwen3-32b',                    turn:0, accepted:false },
];
const MAX_TURNS = 3;
const ENDPOINT = 'https://api.groq.com/openai/v1/chat/completions';
const MIN_THINK_TIME_MS = 2000;   // <-- 2â€‘second minimum

/* ==================== UTILITIES ==================== */
// tiny sleep helper
function sleep(ms){ return new Promise(r=>setTimeout(r,ms)); }

/* ==================== UI HELPERS ==================== */
const chatEl = document.getElementById('chat');

/**
 * Append a new chat bubble.
 * @param {string} text  Markdownâ€‘formatted text.
 * @param {'user'|'bot'} from  Who sent it.
 * @param {string} [model]  Optional model name to show above the bubble.
 */
function addMessage(text, from='bot', model='') {
  const div = document.createElement('div');
  div.className = `msg ${from}`;

  // Convert Markdown â†’ safe HTML (using marked.js)
  const html = marked.parse(text, {
    gfm: true,
    breaks: true,
    smartLists: true
  });

  if (model) {
    div.innerHTML = `<div class="model">${model}</div>` + html;
  } else {
    div.innerHTML = html;
  }

  chatEl.appendChild(div);
  chatEl.scrollTop = chatEl.scrollHeight;
}

/**
 * Show the final conclusion after the team finishes.
 */
function setFinal(text){
  const fin = document.createElement('div');
  fin.className='final';
  const html = marked.parse(text, {
    gfm: true,
    breaks: true,
    smartLists: true
  });
  fin.innerHTML = `<strong>Final Conclusion:</strong> ${html}`;
  chatEl.appendChild(fin);
}

/* ==================== GROQ CALL ==================== */
async function callModel(model, messages){
  const payload = {
    model,
    messages,
    temperature:0.2,
    max_tokens:1024,
    stream:false
  };

  try {
    const res = await fetch(ENDPOINT,{
      method:'POST',
      headers:{
        'Content-Type':'application/json',
        'Authorization':`Bearer ${GROQ_KEY}`
      },
      body:JSON.stringify(payload)
    });

    // ------------ RATEâ€‘LIMIT HANDLING ------------
    if (res.status === 429) {
      const retryAfter = res.headers.get('Retry-After') || 'a few minutes';
      const errMsg = `ðŸš¨ Rate limit exceeded (429). Please wait ${retryAfter} before trying again.`;
      addMessage(errMsg, 'bot');
      throw new Error('RATE_LIMIT');
    }

    if (!res.ok) {
      const errBody = await res.text();
      throw new Error(`API error ${res.status}: ${errBody}`);
    }

    const data = await res.json();
    return data.choices[0].message.content.trim();

  } catch (e) {
    if (e.message !== 'RATE_LIMIT') {
      addMessage(`âš ï¸ ${e.message}`, 'bot');
    }
    throw e;
  }
}

/* ==================== CORE LOGIC ==================== */
let userQuestion = '';
let conclusion   = '';   // shared answer that gets refined

async function runTeamLoop(){
  // reset perâ€‘question state
  MODELS.forEach(m=>{m.turn=0; m.accepted=false;});
  conclusion = '';
  let anyChange = true;

  try {
    while(anyChange){
      anyChange = false;

      for(const bot of MODELS){
        if(bot.accepted) continue;
        if(bot.turn >= MAX_TURNS) continue;

        bot.turn++;
        const turnInfo = ` (turn ${bot.turn}/${MAX_TURNS})`;
        addMessage(`Thinking${turnInfo}â€¦`, 'bot', bot.name);

        // ----------- ENSURE MINIMUM THINKING TIME ------------
        const thinkStart = Date.now();

        // ---------- Prompt construction ----------
        let systemPrompt, userPrompt;
        if(conclusion===''){
          systemPrompt = `You are a helpful AI assistant. Answer the user's question as concisely and completely as possible.`;
          userPrompt   = `Question: ${userQuestion}`;
        }else{
          systemPrompt = `You are a reviewer AI. Read the provided answer for question: '${userQuestion}'. If it is correct, reply with the single word "ACCEPT". If you find any mistake or missing detail, reply with a corrected version of the answer (no need to say "CORRECTED", just give the improved answer). Be brief but accurate.`;
          userPrompt   = `Current answer:\n${conclusion}\n\nYour response:`;
        }

        // ---------- Model call ----------
        const response = await callModel(bot.name, [
          {role:'system', content:systemPrompt},
          {role:'user',   content:userPrompt}
        ]);

        // Wait the remaining time if the API was faster than MIN_THINK_TIME_MS
        const elapsed = Date.now() - thinkStart;
        if (elapsed < MIN_THINK_TIME_MS) {
          await sleep(MIN_THINK_TIME_MS - elapsed);
        }

        console.log(`[${bot.name}] raw response:`, response);
        addMessage(response, 'bot', bot.name);

        // ---------- Interpretation ----------
        if(conclusion===''){
          conclusion = response;
          anyChange = true;
          continue;
        }

        const firstWord = response.split(/\s+/)[0].toUpperCase();
        if(firstWord === 'ACCEPT'){
          bot.accepted = true;
          addMessage(`âœ… ${bot.name} ACCEPTED the answer.`, 'bot');
        }else{
          conclusion = response;
          addMessage(`ðŸ”§ ${bot.name} provided a correction. Updated conclusion.`, 'bot');
          anyChange = true;
        }
      }

      // stop when everyone has accepted or all have exhausted their turns
      if (MODELS.every(b=>b.accepted)) break;
      if (MODELS.every(b=>b.turn>=MAX_TURNS)) break;
    }

    setFinal(conclusion || 'No conclusion could be reached.');

  } catch (e){
    console.warn('Team loop halted:', e);
  }
}

/* ==================== EVENT LISTENERS ==================== */
document.getElementById('sendBtn').addEventListener('click', async ()=>{
  const inp = document.getElementById('questionInput');
  const q   = inp.value.trim();
  if(!q) return;
  userQuestion = q;
  addMessage(q, 'user');
  inp.value = '';
  await runTeamLoop();
});

document.getElementById('questionInput').addEventListener('keypress', e=>{
  if(e.key==='Enter') document.getElementById('sendBtn').click();
});
</script>

<!-- Marked.js â€“ Markdown parser -->
<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
</body>
</html>